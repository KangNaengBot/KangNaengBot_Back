"""
ê°•ë‚¨ëŒ€í•™êµ êµìˆ˜ì •ë³´ JSONL ë°ì´í„°ë¥¼ Vertex AI RAG ì½”í¼ìŠ¤ì— ì—…ë¡œë“œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python google_adk/data/êµìˆ˜ì •ë³´/upload_professors_to_rag.py
"""

import json
import os
from pathlib import Path
import vertexai
from vertexai.preview import rag
from google.cloud import storage

# ==============================
# ì„¤ì •
# ==============================
# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬
SCRIPT_DIR = Path(__file__).parent
PROFESSORS_DIR = SCRIPT_DIR

# ì²˜ë¦¬í•  JSONL íŒŒì¼ë“¤
JSONL_FILES = [
    "ê³µê³¼ëŒ€í•™.jsonl",
    "ê¸€ë¡œë²Œë¬¸í™”ì½˜í…ì¸ ëŒ€í•™.jsonl",
    "ë²•í–‰ì •ì„¸ë¬´í•™ë¶€.jsonl",
    "ì‚¬ë²”ëŒ€í•™.jsonl",
    "ì‚¬íšŒë³µì§€í•™ê³¼.jsonl",
    "ìƒê²½í•™ë¶€.jsonl",
    "ì‹œë‹ˆì–´ë¹„ì¦ˆë‹ˆìŠ¤í•™ê³¼.jsonl",
    "ì˜ˆì²´ëŠ¥ëŒ€í•™.jsonl",
]

# GCS ì„¤ì •
GCS_BUCKET_NAME = "kangnam-univ"
GCS_BUCKET_LOCATION = "asia-northeast3"  # ì„œìš¸
GCS_RAG_DATA_BASE_PATH = "rag_data/professors/"  # GCS ê¸°ë³¸ ê²½ë¡œ

# Vertex AI ì„¤ì •
PROJECT_ID = "kangnam-backend"
LOCATION = "us-east4"

# âš ï¸ ì½”í¼ìŠ¤ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”!
# create_professor_corpus.py ì‹¤í–‰ í›„ ìƒì„±ëœ ì½”í¼ìŠ¤ IDë¡œ êµì²´
CORPUS_ID = "4532873024948404224"  # ğŸ”´ ì—¬ê¸°ì— ìƒˆ ì½”í¼ìŠ¤ ID ì…ë ¥ í•„ìš”!
CORPUS_NAME = f"projects/{PROJECT_ID}/locations/{LOCATION}/ragCorpora/{CORPUS_ID}"

# ==============================
# ì´ˆê¸°í™”
# ==============================
print("=" * 60)
print("ğŸ“ ê°•ë‚¨ëŒ€í•™êµ êµìˆ˜ì •ë³´ RAG ì—…ë¡œë“œ")
print("=" * 60)
print(f"\nğŸ”„ ì´ˆê¸°í™” ì¤‘...")
print(f"   í”„ë¡œì íŠ¸: {PROJECT_ID}")
print(f"   Vertex AI ë¦¬ì „: {LOCATION}")
print(f"   GCS ë²„í‚·: gs://{GCS_BUCKET_NAME}")
print(f"   êµìˆ˜ì •ë³´ í´ë”: {PROFESSORS_DIR}")

# ì½”í¼ìŠ¤ ID í™•ì¸
if CORPUS_ID == "YOUR_NEW_CORPUS_ID":
    print("\n" + "=" * 60)
    print("âš ï¸  ìƒˆ ì½”í¼ìŠ¤ë¥¼ ë¨¼ì € ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤!")
    print("=" * 60)
    print("\nì•„ë˜ ëª…ë ¹ì–´ë¡œ ìƒˆ ì½”í¼ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš”:\n")
    print("ğŸ“Œ ì½”í¼ìŠ¤ ìƒì„±:")
    print("-" * 60)
    print(f"   python google_adk/data/êµìˆ˜ì •ë³´/create_professor_corpus.py")
    print("-" * 60)
    print("\nìƒì„± í›„:")
    print("   1. ì¶œë ¥ëœ ì½”í¼ìŠ¤ IDë¥¼ ë³µì‚¬")
    print("   2. ì´ ìŠ¤í¬ë¦½íŠ¸ì˜ CORPUS_ID ë³€ìˆ˜ì— ì…ë ¥")
    print("   3. ìŠ¤í¬ë¦½íŠ¸ ë‹¤ì‹œ ì‹¤í–‰\n")
    exit(1)

print(f"   ì½”í¼ìŠ¤ ID: {CORPUS_ID}\n")

# Vertex AI ì´ˆê¸°í™”
vertexai.init(project=PROJECT_ID, location=LOCATION)

# GCS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
storage_client = storage.Client(project=PROJECT_ID)
bucket = storage_client.bucket(GCS_BUCKET_NAME)

# ==============================
# êµìˆ˜ì •ë³´ íŒŒì¼ í™•ì¸
# ==============================
print("ğŸ“ êµìˆ˜ì •ë³´ íŒŒì¼ í™•ì¸ ì¤‘...\n")

available_files = []
missing_files = []

for jsonl_file in JSONL_FILES:
    file_path = PROFESSORS_DIR / jsonl_file
    if file_path.exists():
        available_files.append(jsonl_file)
        with open(file_path, "r", encoding="utf-8") as f:
            line_count = sum(1 for line in f if line.strip())
        print(f"   âœ… {jsonl_file} ({line_count} í•­ëª©)")
    else:
        missing_files.append(jsonl_file)
        print(f"   âŒ {jsonl_file} (íŒŒì¼ ì—†ìŒ)")

if missing_files:
    print(f"\nâš ï¸  {len(missing_files)}ê°œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("   ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")

if not available_files:
    print("\nâŒ ì—…ë¡œë“œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

print(f"\nğŸ“Š ì´ {len(available_files)}ê°œ íŒŒì¼ ì¤€ë¹„ ì™„ë£Œ\n")

# ==============================
# í†µê³„ ìˆ˜ì§‘
# ==============================
print("ğŸ“ˆ ë°ì´í„° í†µê³„ ìˆ˜ì§‘ ì¤‘...\n")

total_professors = 0
total_indexes = 0
stats_by_college = {}

for jsonl_file in available_files:
    file_path = PROFESSORS_DIR / jsonl_file
    
    professors = 0
    indexes = 0
    
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            try:
                data = json.loads(line)
                if data.get("metadata", {}).get("entity") == "org_index":
                    indexes += 1
                elif data.get("metadata", {}).get("professor_id"):
                    professors += 1
            except json.JSONDecodeError:
                continue
    
    college_name = jsonl_file.replace(".jsonl", "")
    stats_by_college[college_name] = {
        "professors": professors,
        "indexes": indexes,
        "total": professors + indexes
    }
    
    total_professors += professors
    total_indexes += indexes
    
    print(f"   {college_name}:")
    print(f"      êµìˆ˜: {professors}ëª…")
    print(f"      ì¸ë±ìŠ¤: {indexes}ê°œ")
    print(f"      í•©ê³„: {professors + indexes}ê°œ")

print(f"\n" + "=" * 60)
print(f"   ì „ì²´ êµìˆ˜: {total_professors}ëª…")
print(f"   ì „ì²´ ì¸ë±ìŠ¤: {total_indexes}ê°œ")
print(f"   ì´ í•­ëª©: {total_professors + total_indexes}ê°œ")
print("=" * 60)

# ì‚¬ìš©ì í™•ì¸
print("\n")
confirm = input("ğŸš€ GCSì— ì—…ë¡œë“œí•˜ê³  RAG ì½”í¼ìŠ¤ì— import í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): ").strip().lower()
if confirm not in ['yes', 'y']:
    print("âŒ ì—…ë¡œë“œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    exit(0)

# ==============================
# GCSì— íŒŒì¼ ì—…ë¡œë“œ
# ==============================
print("\nâ˜ï¸  GCSì— ì—…ë¡œë“œ ì¤‘...\n")

uploaded_uris = []

for jsonl_file in available_files:
    file_path = PROFESSORS_DIR / jsonl_file
    gcs_path = f"{GCS_RAG_DATA_BASE_PATH}{jsonl_file}"
    
    print(f"   ğŸ“¤ {jsonl_file} ì—…ë¡œë“œ ì¤‘...")
    
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
        
        blob = bucket.blob(gcs_path)
        blob.upload_from_string(
            content,
            content_type="application/x-ndjson"
        )
        
        gcs_uri = f"gs://{GCS_BUCKET_NAME}/{gcs_path}"
        uploaded_uris.append(gcs_uri)
        
        size_kb = len(content.encode('utf-8')) / 1024
        print(f"      âœ… ì™„ë£Œ ({size_kb:.2f} KB)")
        print(f"      ğŸ“ {gcs_uri}")
        
    except Exception as e:
        print(f"      âŒ ì‹¤íŒ¨: {str(e)}")
        continue

if not uploaded_uris:
    print("\nâŒ ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

print(f"\nâœ… GCS ì—…ë¡œë“œ ì™„ë£Œ!")
print(f"   ğŸ“¦ {len(uploaded_uris)}ê°œ íŒŒì¼ ì—…ë¡œë“œë¨\n")

# ==============================
# Vertex AI RAG ì½”í¼ìŠ¤ë¡œ Import
# ==============================
print("ğŸš€ Vertex AI RAG ì½”í¼ìŠ¤ì— Import ì¤‘...\n")
print(f"   ì½”í¼ìŠ¤: {CORPUS_NAME}")
print(f"   íŒŒì¼ ìˆ˜: {len(uploaded_uris)}ê°œ\n")

try:
    # ImportFilesConfig ëŒ€ì‹  TransformationConfig ì™€ ChunkingConfig ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    config = rag.TransformationConfig(
        chunking_config=rag.ChunkingConfig(
            chunk_size=512,
            chunk_overlap=50
        )
    )

    # 2. âœ… import_files í•¨ìˆ˜ í˜¸ì¶œ ìˆ˜ì •
    # 'import_files_config=' ëŒ€ì‹  'transformation_config=' ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    operation = rag.import_files(
        corpus_name=CORPUS_NAME,
        paths=uploaded_uris,
        transformation_config=config,  # âœ… ì—¬ê¸°ê°€ ìˆ˜ì •ëœ í•µì‹¬
    )
    
    print("   â³ Import ë° ì„ë² ë”© ìƒì„± ì¤‘...")
    print("   (êµìˆ˜ ì •ë³´ê°€ ë§ì•„ 5-10ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    
    try:
        result = operation.result(timeout=600)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
        print(f"\n   âœ… Import ì™„ë£Œ í™•ì¸ë¨!")
    except Exception as result_error:
        print(f"\n   âš ï¸  ì™„ë£Œ í™•ì¸ íƒ€ì„ì•„ì›ƒ (ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì¤‘ì¼ ìˆ˜ ìˆìŒ)")
        print(f"   ì—ëŸ¬: {str(result_error)}")
    
    print("\n" + "=" * 60)
    print("âœ… Import ìš”ì²­ ì™„ë£Œ!")
    print("=" * 60)
    print(f"\nğŸ“Š ì—…ë¡œë“œ ìš”ì•½:")
    print(f"   â€¢ êµìˆ˜: {total_professors}ëª…")
    print(f"   â€¢ ì¸ë±ìŠ¤: {total_indexes}ê°œ")
    print(f"   â€¢ íŒŒì¼: {len(uploaded_uris)}ê°œ")
    print(f"   â€¢ ì½”í¼ìŠ¤: {CORPUS_ID}")
    
    print(f"\nâ˜ï¸  GCS ê²½ë¡œ:")
    for uri in uploaded_uris:
        print(f"   â€¢ {uri}")
    
    print(f"\nğŸ” 5-10ë¶„ í›„ ì½”í¼ìŠ¤ì—ì„œ ê²€ìƒ‰ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
    print(f"\nğŸ’¡ ë§ˆì§€ë§‰ ë‹¨ê³„:")
    print(f"   google_adk/agents/professor/tools/search_tools.py íŒŒì¼ì„ ì—´ì–´")
    print(f"   PROFESSOR_CORPUS_IDë¥¼ ë‹¤ìŒìœ¼ë¡œ êµì²´:")
    print(f"\n   PROFESSOR_CORPUS_ID = \"{CORPUS_ID}\"\n")
    print(f"\nğŸ’¬ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì˜ˆì‹œ:")
    print(f"   â€¢ 'ì¸ê³µì§€ëŠ¥ êµìˆ˜ë‹˜ ì•Œë ¤ì¤˜'")
    print(f"   â€¢ 'ê¹€ì² ì£¼ êµìˆ˜ë‹˜ ì—°êµ¬ì‹¤ ì–´ë””ì•¼?'")
    print(f"   â€¢ 'ê³µê³¼ëŒ€í•™ êµìˆ˜ë‹˜ë“¤ ì•Œë ¤ì¤˜'")
    print(f"   â€¢ 'VR ì „ê³µ êµìˆ˜ë‹˜ ëˆ„êµ¬ì•¼?'\n")
    
except Exception as e:
    print(f"\nâŒ Vertex AI Import ì‹¤íŒ¨:")
    print(f"   {str(e)}")
    print(f"\nğŸ’¡ í™•ì¸ì‚¬í•­:")
    print(f"   1. ì¸ì¦: gcloud auth application-default login")
    print(f"   2. ê¶Œí•œ: Vertex AI User ì—­í• ")
    print(f"   3. API í™œì„±í™”: Vertex AI API")
    print(f"   4. ì½”í¼ìŠ¤ ID: {CORPUS_ID}")
    print(f"   5. GCS íŒŒì¼ ì ‘ê·¼ ê¶Œí•œ")
    exit(1)
