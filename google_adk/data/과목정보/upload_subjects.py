"""
ê°•ë‚¨ëŒ€í•™êµ ê³¼ëª© ì •ë³´ Vertex AI Search ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python google_adk/data/ê³¼ëª©ì •ë³´/upload_subjects.py
"""

from google.api_core.client_options import ClientOptions
from google.cloud import discoveryengine
from google.cloud import storage
from google.api_core.exceptions import AlreadyExists
import os

# ==============================
# ì„¤ì •
# ==============================
PROJECT_ID = "kangnam-backend"
LOCATION = "global"
DATA_STORE_ID = "kangnam-subjects-datastore"
BUCKET_NAME = "kangnam-univ"
GCS_FOLDER = "rag_data/subjects"
LOCAL_FILE = "google_adk/data/ê³¼ëª©ì •ë³´/kangnam_all_2025_2.jsonl"

# GCS ì…ë ¥ URI
GCS_URI = f"gs://{BUCKET_NAME}/{GCS_FOLDER}/{os.path.basename(LOCAL_FILE)}"

def create_data_store():
    """ë°ì´í„° ìŠ¤í† ì–´ ìƒì„± (ì—†ìœ¼ë©´)"""
    print("=" * 60)
    print("ğŸ” ë°ì´í„° ìŠ¤í† ì–´ í™•ì¸ ë° ìƒì„±")
    print("=" * 60)
    
    client_options = (
        ClientOptions(api_endpoint=f"{LOCATION}-discoveryengine.googleapis.com")
        if LOCATION != "global" else None
    )
    client = discoveryengine.DataStoreServiceClient(client_options=client_options)

    parent = client.collection_path(
        project=PROJECT_ID,
        location=LOCATION,
        collection="default_collection",
    )

    data_store = discoveryengine.DataStore(
        display_name="ê°•ë‚¨ëŒ€í•™êµ ê³¼ëª© ì •ë³´ (2025-2)",
        industry_vertical=discoveryengine.IndustryVertical.GENERIC,
        solution_types=[discoveryengine.SolutionType.SOLUTION_TYPE_SEARCH],
        content_config=discoveryengine.DataStore.ContentConfig.NO_CONTENT,
    )

    request = discoveryengine.CreateDataStoreRequest(
        parent=parent,
        data_store_id=DATA_STORE_ID,
        data_store=data_store,
    )

    try:
        operation = client.create_data_store(request=request)
        print(f"ğŸš€ ë°ì´í„° ìŠ¤í† ì–´ ìƒì„± ìš”ì²­ ì¤‘... (ID: {DATA_STORE_ID})")
        response = operation.result(timeout=600)
        print("âœ… ë°ì´í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ!")
    except AlreadyExists:
        print(f"â„¹ï¸ ë°ì´í„° ìŠ¤í† ì–´ '{DATA_STORE_ID}'ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ë°ì´í„° ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def upload_to_gcs():
    """JSONL íŒŒì¼ì„ GCSì— ì—…ë¡œë“œ"""
    print("\n" + "=" * 60)
    print("ğŸ“¤ GCS ì—…ë¡œë“œ ì‹œì‘")
    print("=" * 60)
    
    if not os.path.exists(LOCAL_FILE):
        raise FileNotFoundError(f"ë¡œì»¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {LOCAL_FILE}")

    storage_client = storage.Client(project=PROJECT_ID)
    bucket = storage_client.bucket(BUCKET_NAME)
    
    blob_name = f"{GCS_FOLDER}/{os.path.basename(LOCAL_FILE)}"
    blob = bucket.blob(blob_name)
    
    print(f"ğŸ“ ë¡œì»¬ íŒŒì¼: {LOCAL_FILE}")
    print(f"â˜ï¸  GCS ê²½ë¡œ: gs://{BUCKET_NAME}/{blob_name}")
    
    blob.upload_from_filename(LOCAL_FILE)
    print("âœ… GCS ì—…ë¡œë“œ ì™„ë£Œ!")

def import_documents():
    """Vertex AI Searchì— ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°"""
    print("\n" + "=" * 60)
    print("ğŸ“¥ Vertex AI Search - ë¬¸ì„œ Import ì‹œì‘")
    print("=" * 60)
    
    client_options = (
        ClientOptions(api_endpoint=f"{LOCATION}-discoveryengine.googleapis.com")
        if LOCATION != "global" else None
    )
    client = discoveryengine.DocumentServiceClient(client_options=client_options)

    parent = client.branch_path(
        project=PROJECT_ID,
        location=LOCATION,
        data_store=DATA_STORE_ID,
        branch="default_branch",
    )

    gcs_source = discoveryengine.GcsSource(
        input_uris=[GCS_URI],
        data_schema="custom",
    )

    import_request = discoveryengine.ImportDocumentsRequest(
        parent=parent,
        gcs_source=gcs_source,
        reconciliation_mode=discoveryengine.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL,
        auto_generate_ids=False,
        id_field="id",
    )

    print("ğŸš€ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° ìš”ì²­ ì¤‘...")
    operation = client.import_documents(request=import_request)
    print(f"ğŸ• ì‘ì—… ID: {operation.operation.name}")
    print("â³ ë°ì´í„° ì²˜ë¦¬ ì¤‘... (ì•½ 1~3ë¶„ ì†Œìš”)")

    try:
        result = operation.result(timeout=600)
        print("\nâœ… ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ!")
        print(f"ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ë¬¸ì„œ ìˆ˜: {result.import_sample.success_count}")
        if result.import_sample.failure_count > 0:
            print(f"âš ï¸ ì‹¤íŒ¨í•œ ë¬¸ì„œ ìˆ˜: {result.import_sample.failure_count}")
            print(f"ì‹¤íŒ¨ ìƒ˜í”Œ: {result.import_sample.failures}")
    except Exception as e:
        print(f"\nâŒ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    try:
        create_data_store()
        upload_to_gcs()
        import_documents()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print("=" * 60)
    except Exception as e:
        print(f"\nâŒ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
